<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>SpringBoot on 胡说</title>
    <link>https://blog.zhangyingwei.com/tags/springboot/</link>
    <description>Recent content in SpringBoot on 胡说</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-ch</language>
    <lastBuildDate>Tue, 14 May 2024 09:58:26 +0800</lastBuildDate><atom:link href="https://blog.zhangyingwei.com/tags/springboot/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>在数据采集中使用对象池的实践</title>
      <link>https://blog.zhangyingwei.com/posts/2024m5d14h9m58s26/</link>
      <pubDate>Tue, 14 May 2024 09:58:26 +0800</pubDate>
      
      <guid>https://blog.zhangyingwei.com/posts/2024m5d14h9m58s26/</guid>
      <description>在我的日常工作中，有很大精力投入到数据采集上。我需要从 syslog 采集大量数据，通常的流程是，将每条数据进行校验之后解析为对象进行一系列的处理与分析。这会产生大量对象，在 Java 中，大量对象必然意味着大量堆内存和频繁的 GC。为提高对象利用率，降低 GC 压力，我们基于对象池技术进行了一些优化手段。
一、为什么需要对象池 在数据采集系统中，每秒钟可能处理成千上万条日志记录，每条记录都需要转换为对象。频繁的对象创建和销毁会导致较高的性能开销，尤其是增加垃圾回收（GC）的频率，从而影响系统的整体性能。对象池通过复用对象减少创建和销毁的次数，提升性能和资源利用率。
二、对象池的原理 在 Java 中，说到池，我们通常会想到连接池、线程池。实际上，所有的池都是为了解决同一个问题：降低资源重复创建和销毁的频率。
对象池的工作机制与线程池和连接池相似。对象池通过维护一定数量的对象，当需要使用时从池中取出，使用完毕后再归还池中，避免了频繁的对象创建和销毁，显著减少了 GC 的负担。基本原理如下：
 预创建对象：在初始化时，预先创建一组对象或线程，放入池中备用。 获取和归还：需要时从池中取出，使用完毕后归还池中。 复用机制：通过复用已有的对象或线程，避免频繁创建和销毁，提升系统性能。  三、自定义对象池的核心实现 以下是一个自定义对象池在数据采集场景中的实战示例代码：
import java.util.concurrent.BlockingQueue; import java.util.concurrent.LinkedBlockingQueue; public class ObjectPool&amp;lt;T&amp;gt; { private BlockingQueue&amp;lt;T&amp;gt; pool; private int maxPoolSize; private ObjectFactory&amp;lt;T&amp;gt; factory; public ObjectPool(int maxPoolSize, ObjectFactory&amp;lt;T&amp;gt; factory) { this.maxPoolSize = maxPoolSize; this.factory = factory; this.pool = new LinkedBlockingQueue&amp;lt;&amp;gt;(maxPoolSize); initializePool(); } private void initializePool() { for (int i = 0; i &amp;lt; maxPoolSize; i++) { pool.</description>
    </item>
    
    <item>
      <title>SpringBoot 中实现订单过期自动取消</title>
      <link>https://blog.zhangyingwei.com/posts/2024m4d2h10m4s42/</link>
      <pubDate>Fri, 02 Feb 2024 10:04:42 +0800</pubDate>
      
      <guid>https://blog.zhangyingwei.com/posts/2024m4d2h10m4s42/</guid>
      <description>在电商等需要在线支付的应用中，通常需要设置订单自动取消的功能。本文将介绍几种在 Spring Boot 中实现订单 30 分钟自动取消的方案，包括定时任务、延迟队列和 Redis 过期事件。
方案一：定时任务 定时任务是一种简单且常用的实现订单自动取消的方案。在 Spring Boot 中，可以使用注解@Scheduled来定义定时任务，任务会按照指定的时间间隔执行。在这个方案中，我们可以定义一个定时任务，每隔 30 分钟检查一次未支付的订单，如果订单生成时间超过 30 分钟，则自动取消该订单。
代码示例：
import org.springframework.scheduling.annotation.EnableScheduling; import org.springframework.scheduling.annotation.Scheduled; import org.springframework.stereotype.Component; @EnableScheduling @Component public class OrderCancelSchedule { @Autowired private OrderService orderService; @Scheduled(cron = &amp;#34;0 0/1 * * *?&amp;#34;) public void cancelUnpaidOrders() { List&amp;lt;Order&amp;gt; unpaidOrders = orderService.getUnpaidOrders(); unpaidOrders.forEach(order -&amp;gt; { if (order.getCreationTime().plusMinutes(30).isBefore(LocalDateTime.now())) { orderService.cancelOrder(order.getId()); } }); } } 在上面的代码中，我们定义了一个名为OrderCancelSchedule的组件，并使用@EnableScheduling注解启用定时任务功能。在组件中，我们定义了一个名为cancelUnpaidOrders的方法，并使用@Scheduled注解来指定该方法作为定时任务执行。cron表达式&amp;quot;0 0/1 * * *?&amp;quot;表示任务每隔 1 分钟执行一次。
方案二：延迟队列 延迟队列是一种将任务延迟执行的机制，入队的元素在一定的延迟时间之后才能出队。在这个方案中，我们可以将订单的 ID 放入延迟队列中，并设置延迟时间为 30 分钟。当延迟时间到期时，从队列中取出订单 ID，并执行取消订单的操作。</description>
    </item>
    
    <item>
      <title>探索 Kafka 消息丢失的问题和解决方案</title>
      <link>https://blog.zhangyingwei.com/posts/2024m4d2h10m23s15/</link>
      <pubDate>Tue, 02 Jan 2024 10:23:15 +0800</pubDate>
      
      <guid>https://blog.zhangyingwei.com/posts/2024m4d2h10m23s15/</guid>
      <description>在构建基于 Kafka 的消息处理系统中，消息丢失是一个需要深入研究的重要问题。强大的系统不仅依赖于其功能，而且依赖于其可靠性。因此，理解消息丢失的原因，并采取必要的措施确保消息的一致性和完整性，是构建高效可靠消息系统的重要组成部分。本文将详细分析 Kafka 消息丢失的主要原因，并提供一系列策略来解决这个问题。
消息丢失的原因  生产者端问题： 在 Kafka 系统中，生产者负责发送消息。然而，由于网络故障或其他未知问题，生产者可能无法成功发送消息到 Kafka 服务器。 Kafka 服务端问题： Kafka 服务器可能会因为硬件故障、磁盘满或其他异常情况导致消息丢失。 消费者端问题： 消费者负责处理接收到的消息。但是，消费者在处理消息时可能会出现错误或崩溃，导致消息未被正确处理。  解决方案与措施 生产者端相关方案与措施  发送消息处理回调方法  由于消息的常规发送采用的异步方式，所以通常会忽略掉回调处理，为了保证消息的发送质量，一定需要对回调信息进行处理或者改为同步发送。
producer.send(new ProducerRecord&amp;lt;&amp;gt;(topic, messageKey, messageStr), new CallBack({...}); 设置有效的重试策略以及 acks 配置  我们可以在生产者端设置一个有效的重试策略，保证消息成功发送。例如，我们可以使用指数退避算法进行重试。这种算法会在每次重试失败后等待更长的时间，从而减轻服务器的压力，并增加消息成功发送的概率。
通过设置 Producer acks 机制，我们可以确保生产者收到 Kafka 服务器的确认，知晓消息是否被成功提交。
 acks=0： 生产者在发送消息后不会等待任何确认，直接将消息视为发送成功。这种设置下，可能会出现消息丢失的情况，因为生产者不会等待服务器的任何确认即认为消息发送成功。 acks=1： 生产者在发送消息后会等待 Leader Broker 的确认，确认后即视为消息发送成功。这种设置下，消息的可靠性得到一定程度的保证，但仍有可能发生 Leader Broker 宕机导致消息丢失的情况。 acks=all： 生产者在发送消息后会等待 Leader Broker 和所有副本的确认，确认后才视为消息发送成功。这种设置下，消息的可靠性和一致性得到最高级别的保证，但同时也会增加网络延迟和资源消耗。  import org.apache.kafka.clients.producer.*; import org.apache.kafka.common.serialization.StringSerializer; import java.util.Properties; public class KafkaProducerExample { private static final String TOPIC_NAME = &amp;#34;my-topic&amp;#34;; private static final String BOOTSTRAP_SERVERS = &amp;#34;localhost:9092&amp;#34;; public static void main(String[] args) { Properties props = new Properties(); props.</description>
    </item>
    
  </channel>
</rss>
